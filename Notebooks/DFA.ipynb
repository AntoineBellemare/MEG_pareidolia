{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import mne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import scipy.io as sio\n",
    "import sys\n",
    "from scipy.signal import hilbert\n",
    "import h5py\n",
    "import neurokit2 as nk\n",
    "import pandas as pd\n",
    "import antropy as ant\n",
    "sys.path.insert(0, \"C:/Users/Antoine/github/MEG_pareidolia/python_scripts/Functions\")\n",
    "from MEG_pareidolia_utils import *\n",
    "import PARAMS\n",
    "from PARAMS import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_LIST = {\n",
    "        \"pareidolia\": [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"],\n",
    "        \"RS\": [\"1\", \"2\"],\n",
    "    }\n",
    "SUBJ_LIST = [\"00\", \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\"]\n",
    "TASK_LIST = [\"pareidolia\"]\n",
    "\n",
    "def process_trial(\n",
    "    subj, task, run, trial, hilbert_data, freq_band_idx, FREQ_BANDS\n",
    "):\n",
    "    amplitude_envelope = hilbert_data\n",
    "    dfa_results = []\n",
    "    for electrode_index, envelope in enumerate(amplitude_envelope):\n",
    "        envelope = rescale_array(envelope, -1, 1)\n",
    "        #dfa_exponent, info = nk.fractal_dfa(envelope)\n",
    "        dfa_exponent = ant.detrended_fluctuation(envelope)\n",
    "        print(\"DFA exponent\", dfa_exponent)\n",
    "        freq_band_name = FREQ_BANDS[freq_band_idx]\n",
    "        print(\"freq band name\", freq_band_name)\n",
    "        dfa_results.append(\n",
    "            {\n",
    "                \"Subject\": subj,\n",
    "                \"Task\": task,\n",
    "                \"Run\": run,\n",
    "                \"Trial\": trial,\n",
    "                \"Frequency_Band\": freq_band_name,\n",
    "                \"Electrode\": electrode_index,\n",
    "                \"DFA_Exponent\": dfa_exponent,\n",
    "            }\n",
    "        )\n",
    "    return dfa_results\n",
    "\n",
    "\n",
    "def rescale_array(arr, new_min, new_max):\n",
    "    min_arr = np.min(arr)\n",
    "    max_arr = np.max(arr)\n",
    "    scaled_array = new_min + (\n",
    "        (arr - min_arr) * (new_max - new_min) / (max_arr - min_arr)\n",
    "    )\n",
    "    return scaled_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = '00'\n",
    "task = 'pareidolia'\n",
    "run = 2\n",
    "\n",
    "hilbert_file, hilbert_path = get_pareidolia_bids(\n",
    "                FOLDERPATH, subj, task, run, stage=\"Hilbert_long\"\n",
    "            )\n",
    "with h5py.File(hilbert_path, \"r\") as f:\n",
    "    hilbert_data = f[\"hilbert_data\"][:]\n",
    "hilbert_data = hilbert_data[:, 0,:, :,  :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = pd.read_csv('df_ALL_metadata_MEG_sub00to11_epo_long_last.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove these columns 'DFA_delta', 'DFA_theta', 'DFA_alpha', 'DFA_low_beta', 'DFA_high_beta' 'DFA_gamma1', 'DFA_gamma2'\n",
    "\n",
    "main_df = main_df.drop(['DFA_delta', 'DFA_theta', 'DFA_alpha', 'DFA_low_beta', 'DFA_high_beta', 'DFA_gamma1', 'DFA_gamma2'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store all DFA DataFrames\n",
    "all_dfa_dfs = []\n",
    "list_subj = ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11']\n",
    "run_list = [1, 2, 3, 4, 5, 6]\n",
    "# Loop through each subject and run to load DFA DataFrames\n",
    "for subj in list_subj:\n",
    "    for run in run_list:\n",
    "        try:\n",
    "            # Construct the path to the saved DFA DataFrame\n",
    "            _, dfa_path = get_pareidolia_bids(FOLDERPATH, subj, task, run, stage=\"DFA_all_bands\")\n",
    "            \n",
    "            # Load the DFA DataFrame\n",
    "            dfa_df = pd.read_csv(dfa_path)\n",
    "\n",
    "            # Rename columns to match main_df\n",
    "            dfa_df.rename(columns={'Subject': 'participant', \n",
    "                                'Run': 'bloc', \n",
    "                                'Trial': 'trials', \n",
    "                                'Electrode': 'electrodes'}, inplace=True)\n",
    "\n",
    "            # Add the loaded DataFrame to the list\n",
    "            all_dfa_dfs.append(dfa_df)\n",
    "        except FileNotFoundError:\n",
    "            print(f'File not found for subject {subj} and run {run}')\n",
    "\n",
    "# Assuming all_dfa_dfs is a list of your loaded and column-renamed DFA DataFrames\n",
    "concatenated_dfa_df = pd.concat(all_dfa_dfs)\n",
    "\n",
    "# Drop duplicates from both the main and DFA DataFrames\n",
    "main_df.drop_duplicates(subset=['participant', 'bloc', 'trials', 'electrodes'], inplace=True)\n",
    "concatenated_dfa_df.drop_duplicates(subset=['participant', 'bloc', 'trials', 'electrodes'], inplace=True)\n",
    "\n",
    "# Ensure that the indexes are reset\n",
    "main_df.reset_index(drop=True, inplace=True)\n",
    "concatenated_dfa_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Merge the DataFrames\n",
    "merged_df = pd.merge(main_df, concatenated_dfa_df, on=['participant', 'bloc', 'trials', 'electrodes'], how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('df_ALL_metadata_MEG_sub00to11_epo_long_last.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = pd.read_csv('df_ALL_metadata_MEG_sub00to11_epo_long_Higuchi_DFA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store all DFA DataFrames\n",
    "all_higuchi_dfs = []\n",
    "list_subj = ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11']\n",
    "run_list = [1, 2, 3, 4, 5, 6]\n",
    "# Loop through each subject and run to load DFA DataFrames\n",
    "for subj in list_subj:\n",
    "    for run in run_list:\n",
    "        try:\n",
    "            # Construct the path to the saved DFA DataFrame\n",
    "            _, dfa_path = get_pareidolia_bids(FOLDERPATH, subj, task, run, stage=\"array_comp_higuchi\")\n",
    "\n",
    "            # Load the DFA DataFrame\n",
    "            dfa_df = np.load(dfa_path+'.npy')\n",
    "\n",
    "            # Create DF from the array (trials, channels) with one column for electrode, one for trial and one for higuchi\n",
    "            higuchi_df = pd.DataFrame(dfa_df, columns=[i for i in range(dfa_df.shape[1])])\n",
    "            higuchi_df['trials'] = higuchi_df.index\n",
    "            \n",
    "            higuchi_df = pd.melt(higuchi_df, id_vars=['trials'], var_name='electrodes', value_name='Higuchi')\n",
    "            higuchi_df['participant'] = subj\n",
    "            higuchi_df['bloc'] = run\n",
    "            all_higuchi_dfs.append(higuchi_df)\n",
    "        except FileNotFoundError:\n",
    "            print(f'File not found for subject {subj} and run {run}')\n",
    "\n",
    "# Assuming all_dfa_dfs is a list of your loaded and column-renamed DFA DataFrames\n",
    "concatenated_df = pd.concat(all_higuchi_dfs)\n",
    "\n",
    "# Drop duplicates from both the main and DFA DataFrames\n",
    "main_df.drop_duplicates(subset=['participant', 'bloc', 'trials', 'electrodes'], inplace=True)\n",
    "concatenated_df.drop_duplicates(subset=['participant', 'bloc', 'trials', 'electrodes'], inplace=True)\n",
    "\n",
    "# Ensure that the indexes are reset\n",
    "main_df.reset_index(drop=True, inplace=True)\n",
    "concatenated_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Convert columns to integers in both DataFrames\n",
    "\n",
    "main_df['participant'] = main_df['participant'].astype(int)\n",
    "main_df['bloc'] = main_df['bloc'].astype(int)\n",
    "main_df['trials'] = main_df['trials'].astype(int)\n",
    "main_df['electrodes'] = main_df['electrodes'].astype(int)\n",
    "\n",
    "concatenated_df['participant'] = concatenated_df['participant'].astype(int)\n",
    "concatenated_df['bloc'] = concatenated_df['bloc'].astype(int)\n",
    "concatenated_df['trials'] = concatenated_df['trials'].astype(int)\n",
    "concatenated_df['electrodes'] = concatenated_df['electrodes'].astype(int)\n",
    "\n",
    "# Now merge the DataFrames\n",
    "merged_df = pd.merge(main_df, concatenated_df, on=['participant', 'bloc', 'trials', 'electrodes'], how='inner')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('df_ALL_metadata_MEG_sub00to11_epo_long_Higuchi_DFA.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(100, 1000)\n",
    "\n",
    "# take last 100 elements of the second dimension\n",
    "x = x[:, -100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# ... (other imports)\n",
    "\n",
    "list_subj = [\"00\", \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\"]\n",
    "task = \"pareidolia\"\n",
    "run_list = [1, 2, 3, 4, 5, 6]\n",
    "FREQ_BANDS_NAMES = [\"delta\", \"theta\", \"alpha\", \"low_beta\", \"high_beta\", \"gamma1\", \"gamma2\"]\n",
    "sf=1200\n",
    "for subj in list_subj:\n",
    "    for run in run_list:\n",
    "        try:\n",
    "            all_band_results = []  # Store results for all bands in the current run\n",
    "            \n",
    "            hilbert_file, hilbert_path = get_pareidolia_bids(\n",
    "                            FOLDERPATH, subj, task, run, stage=\"Hilbert_RT\"\n",
    "                        )\n",
    "            with h5py.File(hilbert_path, \"r\") as f:\n",
    "                hilbert_data = f[\"hilbert_data\"][:]\n",
    "            hilbert_data = hilbert_data[:, 0,:, :,  :]\n",
    "            # keep the last 8 seconds in the last dimension\n",
    "            n_points = 8 *sf\n",
    "            hilbert_data = hilbert_data[:, :, :, -n_points:]\n",
    "            for band_index in range(len(FREQ_BANDS)):\n",
    "                pbar = tqdm(total=hilbert_data.shape[1])  # Initialize progress bar\n",
    "\n",
    "                # Process each trial for the current band\n",
    "                for trial in range(hilbert_data.shape[1]):\n",
    "                    result = process_trial(subj, task, run, trial, hilbert_data[band_index, trial, :, :], band_index, FREQ_BANDS)\n",
    "                    all_band_results.extend(result)  # Accumulate results\n",
    "                    pbar.update(1)  # Update progress bar\n",
    "\n",
    "                pbar.close()  # Close the progress bar\n",
    "\n",
    "            # Convert the results to a DataFrame\n",
    "            dfa_df = pd.DataFrame(all_band_results)\n",
    "\n",
    "            dfa_df[\"Frequency_Band\"] = dfa_df[\"Frequency_Band\"].apply(lambda x: x[0])\n",
    "            dfa_pivoted = dfa_df.pivot_table(index=[\"Subject\", \"Task\", \"Run\", \"Trial\", \"Electrode\"], \n",
    "                                                columns=\"Frequency_Band\", \n",
    "                                                values=\"DFA_Exponent\").reset_index()\n",
    "\n",
    "            # Rename columns to have DFA_band names\n",
    "            dfa_pivoted.columns = ['Subject', 'Task', 'Run', 'Trial', 'Electrode'] + [f'DFA_{band}' for band in FREQ_BANDS_NAMES]\n",
    "            # Save the pivoted DataFrame\n",
    "            dfa_file, dfa_path = get_pareidolia_bids(FOLDERPATH, subj, task, run, stage=\"DFA_all_bands_RT\")\n",
    "            dfa_pivoted.to_csv(dfa_path, index=False)\n",
    "            del hilbert_data\n",
    "        except FileNotFoundError:\n",
    "            print('FILENOTFOUND', subj, run)\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Other necessary imports and function definitions\n",
    "\n",
    "def process_run(run, FREQ_BANDS, band_index, subject, task, FOLDERPATH):\n",
    "    hilbert_file, hilbert_path = get_pareidolia_bids(\n",
    "                FOLDERPATH, subject, task, run, stage=\"Hilbert_long\"\n",
    "            )\n",
    "    with h5py.File(hilbert_path, \"r\") as f:\n",
    "        hilbert_data = f[\"hilbert_data\"][:]\n",
    "    hilbert_data = hilbert_data[:, 0,:, :,  :]\n",
    "    # Initialize the progress bar for this run\n",
    "    total_trials = hilbert_data.shape[1]\n",
    "    pbar = tqdm(total=total_trials, desc=f\"Processing Run {run}\")\n",
    "\n",
    "    min_freq = FREQ_BANDS[band_index][0]\n",
    "    max_freq = FREQ_BANDS[band_index][1]\n",
    "    results = []\n",
    "\n",
    "    # Process each trial for the specified band\n",
    "    for trial in range(hilbert_data.shape[1]):\n",
    "        result = process_trial(subject, task, run, trial, hilbert_data[band_index, trial, :, :], min_freq, max_freq)\n",
    "        results.append(result)\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # Save results for this run\n",
    "    dfa_df = pd.DataFrame(results)\n",
    "    dfa_file, dfa_path = get_pareidolia_bids(FOLDERPATH, subject, task, run, stage=\"DFA_alpha\")\n",
    "    dfa_df.to_csv(dfa_path, index=False)\n",
    "\n",
    "def main():\n",
    "    subject = \"00\"\n",
    "    task = \"pareidolia\"\n",
    "    run_list = [2, 3, 4, 5]\n",
    "    band_index = 2  # Alpha band\n",
    "\n",
    "    # Create a pool of processes\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        pool.starmap(process_run, [(run, FREQ_BANDS, band_index, subject, task, FOLDERPATH) for run in run_list])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming other necessary imports and function definitions (like process_trial) are done above\n",
    "\n",
    "subject = \"00\"\n",
    "task = \"pareidolia\"\n",
    "run = 1\n",
    "band_index = 2  # Alpha band\n",
    "\n",
    "# Assuming hilbert_data is loaded properly\n",
    "\n",
    "# Create a pool of worker processes\n",
    "pool = multiprocessing.Pool(processes=multiprocessing.cpu_count() // 2)\n",
    "results = []\n",
    "\n",
    "# Initialize the progress bar\n",
    "total_trials = hilbert_data.shape[1]\n",
    "pbar = tqdm(total=total_trials)\n",
    "\n",
    "min_freq = FREQ_BANDS[band_index][0]\n",
    "max_freq = FREQ_BANDS[band_index][1]\n",
    "\n",
    "# Process each trial for the specified band\n",
    "for trial in range(hilbert_data.shape[1]):\n",
    "    result = pool.apply_async(\n",
    "        process_trial,\n",
    "        args=(subject, task, run, trial, hilbert_data[band_index, trial, :, :], min_freq, max_freq)\n",
    "    )\n",
    "    results.append(result)\n",
    "\n",
    "# Wait for all tasks to complete\n",
    "for result in results:\n",
    "    result.wait()\n",
    "    pbar.update(1)  # Update progress bar for each completed task\n",
    "\n",
    "# Close the progress bar and the pool\n",
    "pbar.close()\n",
    "#pool.close()\n",
    "#pool.join()\n",
    "\n",
    "# Gather results\n",
    "dfa_results = [result.get() for result in results]\n",
    "\n",
    "# Flatten the list of lists into a single list\n",
    "dfa_results_flat = [item for sublist in dfa_results for item in sublist]\n",
    "\n",
    "# Process and save results\n",
    "dfa_df = pd.DataFrame(dfa_results_flat)\n",
    "dfa_file, dfa_path = get_pareidolia_bids(FOLDERPATH, subject, task, run, stage=\"DFA_alpha\")\n",
    "dfa_df.to_csv(dfa_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a pool of worker processes\n",
    "manager = multiprocessing.Manager()\n",
    "progress_counter = manager.Value(\"i\", 0)\n",
    "\n",
    "pool = multiprocessing.Pool(processes=multiprocessing.cpu_count() // 4)\n",
    "\n",
    "for subj in SUBJ_LIST:\n",
    "    for task in TASK_LIST:\n",
    "        for run in RUN_LIST[task]:\n",
    "            print(\"Run\", run)\n",
    "            # Load Hilbert data\n",
    "            # Load the Hilbert transform data\n",
    "            hilbert_file, hilbert_path = get_pareidolia_bids(\n",
    "                FOLDERPATH, subj, task, run, stage=\"Hilbert_long\"\n",
    "            )\n",
    "            with h5py.File(hilbert_path, \"r\") as f:\n",
    "                hilbert_data = f[\"hilbert_data\"][:]\n",
    "\n",
    "            # save the hilbert data with new name\n",
    "            #hilbert_file, hilbert_path = get_pareidolia_bids(\n",
    "            #    FOLDERPATH, subj, task, run, stage=\"Hilbert\"\n",
    "            #)\n",
    "            #with h5py.File(hilbert_path, \"w\") as f:\n",
    "            #    f.create_dataset(\"hilbert_data\", data=hilbert_data)\n",
    "            \n",
    "            total_tasks = sum(\n",
    "                len(RUN_LIST[task]) * len(FREQ_BANDS) * hilbert_data.shape[2]\n",
    "                for task in TASK_LIST\n",
    "            )\n",
    "            results = []\n",
    "            for i, (min_freq, max_freq) in enumerate(FREQ_BANDS):\n",
    "                for trial in range(hilbert_data.shape[2]):\n",
    "                    result = pool.apply_async(\n",
    "                        process_trial,\n",
    "                        (\n",
    "                            subj,\n",
    "                            task,\n",
    "                            run,\n",
    "                            trial,\n",
    "                            hilbert_data[i, 0, :],\n",
    "                            min_freq,\n",
    "                            max_freq,\n",
    "                            progress_counter,\n",
    "                        ),\n",
    "                    )\n",
    "                    results.append(result)\n",
    "\n",
    "            for r in results:\n",
    "                r.get()\n",
    "                print(\n",
    "                    f\"Progress: {progress_counter.value}/{total_tasks} tasks completed\"\n",
    "                )\n",
    "\n",
    "            # Gather and flatten results\n",
    "            dfa_results = [\n",
    "                item for sublist in [r.get() for r in results] for item in sublist\n",
    "            ]\n",
    "            print(\"DFA results\", dfa_results)\n",
    "\n",
    "            # Convert results to DataFrame and save to CSV\n",
    "            dfa_df = pd.DataFrame(dfa_results)\n",
    "            print(\"DFA df\", dfa_df)\n",
    "            dfa_file, dfa_path = get_pareidolia_bids(\n",
    "                FOLDERPATH, subj, task, run, stage=\"DFA\"\n",
    "            )\n",
    "            dfa_df.to_csv(dfa_path, index=False)\n",
    "\n",
    "# Close the pool and wait for all processes to complete\n",
    "pool.close()\n",
    "pool.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biotuner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
